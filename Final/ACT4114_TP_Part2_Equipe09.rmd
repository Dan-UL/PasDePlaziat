---
output:
  pdf_document:
    df_print: kable
    fig_caption: yes
    includes:
      before_body: "TP-title.tex"
      in_header: "preamble-latex.tex"
---

\centering

\clearpage

\tableofcontents

\justify
\clearpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r paquetages, message=FALSE, eval=TRUE, include=FALSE, echo = FALSE}
### Liste des paquetages
liste.paquetage <- c("ggplot2", "maps", "mice", "graphics", "gridExtra", "caret", "gbm", "xgboost", "iml", "tidyverse", "adabag","randomForest", "rpart", "rpart.plot", "pROC", "cluster", "FactoMineR", "tweedie", "statmod", "FNN")

### On installe les paquetages de la liste qu'on a pas déjà
inst <- liste.paquetage %in% installed.packages()
if(length(liste.paquetage[!inst]) > 0) install.packages(liste.paquetage[!inst])

lapply(liste.paquetage, require, character.only = TRUE)
library(ggplot2)
library(maps)
library(mice)
library(graphics)
library(gridExtra)
library(caret)
library(gbm)
library(xgboost)
library(iml)
library(tidyverse)
library(adabag)
library(randomForest)
library(rpart)
library(rpart.plot)
library(pROC)
library(statmod)
library(tweedie)
library(cluster)
library(FactoMineR)
library(tweedie)
library(statmod)
library(FNN)
```

```{r data import, echo = F}
data <- read.csv("DonnéesTraitées.csv")
# On enleve la varible identité
data <- data[, -c(1)]
# Les variables sous le bon format
data$condominiumIndicator <- as.factor(data$condominiumIndicator)
data$communityRatingSystemDiscount <- as.factor(data$communityRatingSystemDiscount)
data$elevatedBuildingIndicator <- as.factor(data$elevatedBuildingIndicator)
data$locationOfContents <- as.factor(data$locationOfContents)
data$numberOfFloorsInTheInsuredBuilding <- as.factor(data$numberOfFloorsInTheInsuredBuilding)
data$occupancyType <- as.factor(data$occupancyType)
data$primaryResidence <- as.factor(data$primaryResidence)
data$totalCoverage <- data$totalBuildingInsuranceCoverage + data$totalContentsInsuranceCoverage

data <- subset(data, select = -c(totalBuildingInsuranceCoverage, totalContentsInsuranceCoverage))


## 85/15 pour le trainning et test
set.seed(1123581321)
ind.train <- sample(1:nrow(data), 0.85*nrow(data), FALSE)
dat.train <- data[ind.train,]
dat.test <- data[-ind.train,]
```

\newpage

# Introduction

<!-- Henri -->

Le présent rapport vise à répondre à un enjeu majeur pour les assureurs en Californie, qui est l'estimation précise du montant des réclamations associées aux polices d'assurance inondation. Pour ce faire, nous avons ajusté plusieurs modèles prédictifs à l'aide d'un jeu de données provenant de la Federal Emergency Management Agency (FEMA), qui contient des informations sur plus de 2 millions de polices d'assurance inondation à travers les États-Unis. Notre objectif est de déterminer le modèle le plus performant pour prédire le montant des réclamations en comparant la racine carrée de l'erreur quadratique moyenne de six modèles différents : un modèle GLM Tweedie, un modèle des k plus proches voisins, un arbre de régression, un modèle bagging, une forêt aléatoire et un modèle gradient boosting.

Dans un premier temps, ce rapport présente la démarche d'ajustement des modèles prédictifs et les techniques utilisées pour optimiser les hyperparamètres de chacun des modèles. Dans un second temps, la procédure de comparaison entre la performance des modèles et les résultats obtenus, y compris la détermination du modèle prédictif le plus performant, sera détaillée. Enfin, nous conclurons sur les perspectives d'amélioration du modèle prédictif et les limites de notre approche.

\newpage

# Modèle de base

<!-- Félix & Henri -->

Le choix d'un modèle linéaire généralisé Tweedie est justifié dans notre étude pour plusieurs raisons. Tout d'abord, cette distribution est particulièrement adaptée pour modéliser la fréquence et la sévérité combinées des données de sinistres d'inondation, tout en prenant en compte leur nature asymétrique. De plus, cette distribution est paramétrée, ce qui permet de modéliser des relations complexes entre les variables explicatives et les montants de sinistres. Le modèle Tweedie retenu peut facilement intégrer des variables continues et catégorielles, ce qui est pertinent pour caractériser le jeu de données considéré. Enfin, l'interprétation des résultats est facilitée par la structure du modèle linéaire généralisé Tweedie, permettant de mieux comprendre les effets des variables explicatives sur les montants de sinistres. Ainsi, ce modèle servira de base efficace pour établir la prime de référence, dans le but d'évaluer la performance d'autres modèles.

```{r GLM, echo = F, warning=F, include=FALSE}
tweed.p <- 1.5
tweed.p <- tweedie.profile(
  totalAmount ~ .,
  data = dat.train,
  p.vec = seq(1.4, 1.8, 0.1),
  method = "series",
  do.plot = FALSE)$p.max

glm.tweed <- glm(
  totalAmount ~ .,
  data = dat.train,
  family = tweedie(var.power = tweed.p, link.power = 0),
  maxit = 300
)

#summary(glm.tweed)
```

![Valeur de p maximal pour Tweedie](tweed_p.jpg)

```{r GLM pred, echo = F}
glm.pred <- predict(glm.tweed, newdata = dat.test)
#plot(glm.pred, ylab = "Prédictions")
```

![Prédictions selon le GLM Tweedie](glm_pred.jpg)

\clearpage

# Ajustement des modèles

<!-- Collectif -->

## Modèle des k plus proches voisins

<!-- Félix & Henri -->

```{r knn, echo = F}
### Création de data 2 pour éviter de #cancel le data original
data2 <- data

### Transformation de toutes les variables factoriel en valeur numérique
data2$condominiumIndicator <- as.numeric(data$condominiumIndicator)
data2$communityRatingSystemDiscount <- as.numeric(data$communityRatingSystemDiscount)
data2$elevatedBuildingIndicator <- as.numeric(data$elevatedBuildingIndicator)
data2$locationOfContents <- as.numeric(data$locationOfContents)
data2$numberOfFloorsInTheInsuredBuilding <- as.numeric(data$numberOfFloorsInTheInsuredBuilding)
data2$occupancyType <- as.numeric(data$occupancyType)
data2$primaryResidence <- as.numeric(data$primaryResidence)

### Utilisation de seulement les data avec un totalAmount plus grand que 0
data_no_0 <- data2[which(data2$totalAmount > 0), ]

### scaling xdf
data_no_0_sc <- cbind(sapply(data_no_0[, -12], scale), data_no_0[, 12])

### Creating training and testing set
data_no_0_training <- sample(1:nrow(data_no_0), 0.85 * nrow(data_no_0), FALSE)
data_no_0_train <- data_no_0_sc[data_no_0_training, ]
data_no_0_test <- data_no_0_sc[-data_no_0_training, ]

### setting things up for knn.reg
obs_train <- data_no_0_train[, -13]
obs_test <- data_no_0_test[, -13]
res_train <- data_no_0_train[, 13]
res_test <- data_no_0_test[, 13]

### Finding optimal K
n <- length(res_test)
knnpred_mat <- matrix(data = rep(0, 100 * length(res_test)), nrow = 100, ncol = length(res_test))
for(i in 1:100) {
    knnpred_mat[i, ] <- knn.reg(obs_train, obs_test, res_train, k = i)$pred
}
EQM <- numeric(100)
for(i in 1:100){
    EQM[i] <- 1 / n * sum((res_test - knnpred_mat[i, ]) ^ 2)
}
#plot(EQM)
#which.min(EQM) ## donc k optimal est 15
### optimal knn model with the no 0 dataset

pred_knn <- knn.reg(obs_train, obs_test, res_train, k = 15)
pred.knn <- pred_knn$pred
#summary(pred.knn)
#summary(res_test)
```

![Erreur quadratique moyenne selon le k](eqm_knn.jpg)

Pour construire un modèle de régression k-plus proche voisin, nous avons dû tout d'abord transformé nos variables catégorielles en valeurs numériques. Cette étape fut nécessaire, car nous voulions obtenir des chiffres plutôt que des classes. Après cette transformation, nous avons cherché à déterminer le facteur "k" optimal pour obtenir le meilleur modèle possible. Nous avons utilisé la méthode de l'erreur quadratique moyenne (EQM) pour évaluer la performance de notre modèle. Cette mesure est estimée à l'aide de la formule suivante : $$\frac{1}{n}\sum^{n}_{i = 1}[y_i-\hat{f}(x_i)]^2.$$

Nous avons testé différentes valeurs de k allant de 1 à 100 et avons observé que l'EQM augmente après un certain point. Après avoir analysé le graphique, nous avons constaté que le k optimal pour notre modèle était de 15, car c'est à ce point que l'EQM atteint son minimum.

\newpage

## Arbre de décision

<!-- Félix & Henri -->

Dans le contexte de la modélisation de montants de réclamation en assurance inondation, il est important de trouver un modèle prédictif à la fois précis et interprétable. Les arbres de décision sont très utiles dans ce contexte, car ils permettent une interprétation facile des résultats et la visualisation simple des règles de décision. Cependant, les arbres non élagués peuvent être très complexes et sujets à un sur ajustement.

Nous avons donc optimisé le paramètre de complexité pour l'élagage en utilisant une validation croisée LGOCV (*Leave Group Out Cross Validation*) à 10 ensembles, puis en minimisant la racine carrée de l'erreur quadratique moyenne (RMSE). L'optimisation du paramètre de complexité permet de trouver le bon niveau d'élagage pour éviter un surajustement tout en maintenant la précision prédictive du modèle.

```{r arbre de regression, echo=FALSE, warning=FALSE}
set.seed(6969)
fit.control <- trainControl(method="LGOCV",
                            p = 0.9,
                            summaryFunction = defaultSummary,
                            classProbs = F,
                            )

cv.fitted.tree <- caret::train(totalAmount~.,
                                data=dat.train,
                                method="rpart",
                                tuneLength=10,
                                metric="RMSE",
                                trControl=fit.control
                                )
#cv.fitted.tree
#rpart.plot(cv.fitted.tree$finalModel)
```

![Arbre élagué](arbre.jpg)

```{r arbre de regression-pred, echo=FALSE}
treepred <- predict(cv.fitted.tree, newdata = dat.test)
```

\newpage

## *Bagging*

<!-- Isabelle -->

```{r bagging,echo=FALSE}
# set.seed(12323)
# bag <- randomForest(totalAmount~.,
#                     data = dat.train,
#                     sampsize = nrow(dat.train),
#                     mtry = 12,
#                     ntree = 400,
#                     nodesize = 10,
#                     importance = TRUE,
#                     cp = 0)
```

Pour le *bagging*, nous utilisons le nombre de variables explicatives pour déterminer le nombre de variables prises en compte à chaque division de l'arbre de décision, soit 12. L'algorithme utilise également un échantillon avec remplacement de la même taille que l'échantillon des données d'entraînement.

```{r bagging-oob,echo=FALSE}
# df.oob.bag <- data.frame(Tree = 1:400,
#                     RMSE = sqrt(bag$mse))
#
# ggplot(df.oob.bag, aes(x=Tree, y = RMSE ))+
#  theme_minimal()+
#  geom_line()+
#  labs(x = "Nombre d'arbre", title = "RMSE selon le nombre d'arbre pour le bagging")+
#  theme(plot.title = element_text(hjust = 0.5))
```

![REQM selon le nombre d'arbres](bag_nb_arbre.jpg)

Le nombre d'arbres pour le *bagging* est déterminé à l'aide de la racine de l'erreur quadratique moyenne (RMSE), car il s'agit d'un problème de régression. En observant la figure ci-dessus (figure 5), nous constatons que le nombre d'arbres se stabilise vers 200 arbres. Pour diminuer le temps de calcul, nous prenons 200 arbres même si un grand nombre d'arbres ne conduit pas à du surajustement.

Ensuite, l'hyperparamètre `nodesize` sera exploré pour prévenir le surajustement causé par des arbres trop profonds. Pour éviter un temps de calcul excessif, les valeurs manuellement testées pour `nodesize` vont de 0 à 500, en augmentant par bonds de 20.

```{r bagging-nodesize, echo=FALSE}
# set.seed(131)
# ind.val.bag <- sample(1:nrow(dat.train), 0.2*nrow(dat.train), FALSE)
# dat.valid.bag <- dat.train[ind.val.bag,]
# dat.non.valid.bag <- dat.train[-ind.val.bag,]
#
# prev.nodesize.bag <- rep(NA, 25)
# for(i in (1:25)*20){
#   set.seed(4757)
#   bag.node <- randomForest(totalAmount~.,
#                             data = dat.non.valid.bag,
#                             ntree = 200,
#                             mtry = 12,
#                             sampsize = nrow(dat.non.valid.bag),
#                             importance = TRUE,
#                             nodesize = i)
#  
#  bag.node.prev <- predict(bag.node, newdata=dat.valid.bag, type="response")
#  prev.nodesize.bag[i/20] <- RMSE(bag.node.prev,dat.valid.bag$totalAmount)
# }
#
# min.node.bag <- which.min(prev.nodesize.bag)*20
#
# df.node.bag <- data.frame("Nodesize" = (1:25)*20 ,
#                     "RMSE" = prev.nodesize.bag 
#                      )
#
```

```{r bagging-nodesize-rmse, echo=FALSE, warning=FALSE}
# ggplot(df.node.bag, aes(x=Nodesize, y = RMSE ))+
#   theme_minimal() +
#   geom_line() +
#   labs(x = "Nombre d'observations par noeud terminal", title = "RMSE selon le nodesize 
# pour le bagging") +
#  theme(plot.title = element_text(hjust = 0.5),
#        legend.position = "none") +
#  geom_line(aes(x = min.node.bag, color = "red"), linetype = "dashed") +
#    geom_text(aes(x = min.node.bag, y = 8200,
#    label = "nodesize = 80", vjust = 1.5, angle = 90))
```

![REQM selon le nombre minimal d’observations par feuille](bag_node.jpg)

La figure 6 montre que le `nodesize` qui minimise l'RMSE se trouve autour de 80. Nous allons donc tester toutes les valeurs de 60 à 100 pour trouver la valeur optimale de l'hyperparamètre.

```{r bagging-nodesize-tune, echo=FALSE}
# prev.nodesize.tune.bag <- rep(NA, 41)
# for(i in (60:100)){
#   set.seed(4757)
#   bag.node.tune <- randomForest(totalAmount~.,
#                             data = dat.non.valid.bag,
#                             ntree = 200,
#                             mtry = 12,
#                             sampsize = nrow(dat.non.valid.bag),
#                             importance = TRUE,
#                             nodesize = i)
#  
#  bag.node.prev.tune <- predict(bag.node.tune, newdata=dat.valid.bag, type="response")
#  prev.nodesize.tune.bag[i-59] <- RMSE(bag.node.prev.tune,dat.valid.bag$totalAmount)
# }
#
# min.node.tune.bag <- which.min(prev.nodesize.tune.bag)+59
#
# df.node.tune.bag <- data.frame("Nodesize" = 60:100,
#                     "RMSE" = prev.nodesize.tune.bag 
#                     )
```

```{r bagging-nodesize-rmse-tune, echo=FALSE, warning=FALSE}
# ggplot(df.node.tune.bag, aes(x=Nodesize, y = RMSE ))+
#   theme_minimal()+
#   geom_line()+
#   labs(x = "Nombre d'observations par noeud terminal", title = "RMSE selon le nodesize 
# pour le bagging")+
#  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")+
#  geom_line(aes(x = min.node.tune.bag, color = "red"), linetype = "dashed")+
#  geom_text(aes(x = min.node.tune.bag, y = 8170,
#    label = "nodesize = 97"), vjust = 1.5, angle = 90)
```

![REQM selon le nombre minimal d’observations par feuille de 60 à 100](bag_node_tune.jpg)

Dans la figure 7, nous pouvons voir que la valeur de `nodesize` optimale est 97.

Les hyperparamètres optimaux pour le *bagging* sont donc les suivants :

```{r bagging-parametre-final, echo=FALSE}
df.bag.final <- data.frame(
  "Hyperparamètre" = c("Nombre d'arbres", "Nombre d'observation dans les noeuds terminaux"),
  "Valeur" = c(200, 97)
)

knitr::kable(df.bag.final,caption = "Valeurs des hyperparamètres du modèle de bagging final")
```

```{r bagging-model-final, echo=FALSE, cache=TRUE}
set.seed(12323)
bag.final <- randomForest(totalAmount~.,
                    data = dat.train,
                    sampsize = nrow(dat.train),
                    mtry = 12,
                    ntree = 200,
                    nodesize = 97,
                    importance = TRUE,
                    cp = 0)
```

\newpage

## Forêt aléatoire

<!-- Danny -->

```{r forest, cache=TRUE, echo = F}
# set.seed(36738)
# foret <- randomForest(totalAmount~.,
#                       data = dat.train,
#                       nodesize = 10,
#                       mtry = 4,
#                       importance = T,
#                       cp = 0,
#                       sampsize = floor(0.5*nrow(data)),
#                       ntree = 400
#                       )

```

Pour la forêt aléatoire, nous commençons par quatre prédicteurs possibles pour chaque séparation, $i.e. \ m=4$, car $\lfloor 12/3 \rfloor = 4$. Cette valeur correspond à la « règle du pouce » en régression où l'on utilise la partie entière du nombre de valeurs explicatives divisé par 3. De plus, en utilisant une proportion de 50% pour les échantillons *bootstrap*, nous aidons à diminuer la corrélation entre les arbres.

```{r forest-tree, cache=TRUE,echo = F , warning=F,  fig.dim= c(7,3)}
# df.oob <- data.frame(Tree = 1:400,
#                      RMSE = sqrt(foret$mse))
# 
# ggplot(df.oob, aes(x=Tree, y = RMSE ))+
#   theme_minimal()+
#   geom_line()+
#   labs(x = "Nombre d'arbre", title = "RMSE selon le nombre d'arbre")+
#   theme(plot.title = element_text(hjust = 0.5))

```

![REQM selon le nombre d'arbres](forest_nb_tree.jpg)

Étant en régression, la racine de l'erreur quadratique moyenne (REQM) sera utilisée comme mesure de comparaison. Nous remarquons ici (figure 8) que la REQM se stabilise aux alentours de 100-150 arbres, nous utiliserons alors 200 arbres pour l'optimisation des autres hyperparamètres, puisque nous ne pouvons pas surajuster en ayant trop d'arbre avec les forêts aléatoires. Maintenant, nous regardons plus en profondeur le nombre de prédicteurs possible à chaque séparation d'un arbre, la variable `mtry`.

```{r forest-tunning, cache=TRUE, echo=FALSE}
# control <- trainControl(method= "cv", number=5)
# 
# grille <- expand.grid(mtry = 1:12)
# 
# set.seed(984532)
# foret.tuning <- caret::train(totalAmount~.,
#                   data = dat.train,
#                   method = "rf",
#                   metric = "RMSE",
#                   tuneGrid = grille,
#                   trControl = control,
#                   sampsize = floor(0.5*nrow(dat.train)),
#                   ntree = 200,
#                   nodesize = 50)


```

```{r forest-mtry, echo=FALSE, cache=TRUE}
forest.rmse <- c(8848, 8660, 8527, 8457, 8408, 8379, 8366, 8352, 8367, 8356, 8360, 8361)

df.forest.mtry <- t(data.frame(mtry = round(1:12),
                          RMSE = forest.rmse))

knitr::kable(df.forest.mtry,caption = "RMSE par rapport au mtry")

best.mtry <- which.min(forest.rmse)


```

Les résultats de la table 2 ont été obtenus par validation croisée à 5 plis, pour ainsi réduire le biais d'échantillonnage. L'utilisation des `r best.mtry` choix de variables explicatives à chaque noeud minimise la REQM.

Pour éviter un surajustement dû à des arbres inutilement trop profonds, nous devrons ajuster la valeur de `nodesize`, mais il est impossible de le faire directement avec le package `caret`. Puisque le modèle est entraîné sur `r nrow(dat.train)` observations, les valeurs de 500 et moins seront testées et comparées. Pour limiter le temps de calcul, un premier entraînement sera fait par bond de 20.

```{r forest-nodesize, echo = FALSE, cache=TRUE, message=FALSE}
## Création de l'échantillon de validation, 20% des observations d'entraînement
# set.seed(95751)
# ind.val <- sample(1:nrow(dat.train), 0.20*nrow(dat.train), FALSE)
# dat.valid <- dat.train[ind.val,]
# dat.non.valid <- dat.train[-ind.val,]
# 
# # Optimiser nodesize sur l'échantillon de validation de 20 à 1000 par bond de 20
# prev.nodesize.rmse <- rep(NA, 25)
# for(i in (1:25)*20 ){
#   
#   set.seed(897941)
#   foret.node <- randomForest(totalAmount~.,
#                       data = dat.non.valid,
#                       nodesize = i,
#                       mtry = best.mtry,
#                       importance = T,
#                       cp = 0,
#                       sampsize = floor(0.5*nrow(dat.non.valid)),
#                       ntree = 200)
#   
#   foret.node.prev <- predict(foret.node, newdata=dat.valid, type="response")
#   prev.nodesize.rmse[i/20] <- RMSE(foret.node.prev,dat.valid$totalAmount)
#   
# }
# 
# 
# min.node <- which.min(prev.nodesize.rmse)*20
# 
# 
# df.node <- data.frame("Nodesize" = (1:25)*20 ,
#                      "RMSE" = prev.nodesize.rmse 
#                      )

```

```{r forest-nodesize-rmse, fig.dim= c(7,3), echo=FALSE, warning=FALSE}
# ggplot(df.node, aes(x=Nodesize, y = RMSE ))+
#   theme_minimal()+
#   geom_line()+
#   labs(x = "Nombre d'observations par noeud terminal", title = "RMSE selon le nodesize")+
#   theme(plot.title = element_text(hjust = 0.5), legend.position = "none")+
#   geom_line(aes(x = min.node, color = "red"), linetype = "dashed")+
#   annotate(geom = "text",
#            label = paste("nodesize =",as.character(min.node)),
#            x = min.node,
#            y = prev.nodesize.rmse[min.node],
#            vjust = 1.5,
#            angle = 90)

```

![REQM selon le nombre minimal d’observations par feuille](forest_node.jpg)

Dans la figure 9, la valeur minimale de `nodesize` est de 20. Puisque l'analyse précédente a été effectuée par bonds de 20, nous la ferons à nouveau, de manière plus précise, de 1 à 40.

```{r forest-nodesize-tune, echo = FALSE, cache=TRUE, message=FALSE}
# # Optimiser nodesize sur l'échantillon de validation de 1 a 40
# prev.nodesize.rmse.tune <- rep(NA, 40)
# for(i in 1:40 ){
#   
#   set.seed(897941)
#   foret.node.tune <- randomForest(totalAmount~.,
#                       data = dat.non.valid,
#                       nodesize = i,
#                       mtry = best.mtry,
#                       importance = T,
#                       cp = 0,
#                       sampsize = floor(0.5*nrow(dat.non.valid)),
#                       ntree = 200)
#   
#   foret.node.prev.tune <- predict(foret.node.tune, newdata=dat.valid, type="response")
#   prev.nodesize.rmse.tune[i] <- RMSE(foret.node.prev.tune,dat.valid$totalAmount)
#   
# }
# 
# 
# min.node.tune <- which.min(prev.nodesize.rmse.tune)
# 
# 
# df.node.tune <- data.frame("Nodesize" = 1:40 ,
#                      "RMSE" = prev.nodesize.rmse.tune 
#                      )
```

```{r forest-nodesize-rmse-tune, fig.dim= c(7,3), echo=FALSE, warning=FALSE}
# ggplot(df.node.tune, aes(x=Nodesize, y = RMSE ))+
#   theme_minimal()+
#   geom_line()+
#   labs(x = "Nombre d'observations par noeud terminal", title = "RMSE selon le nodesize")+
#   theme(plot.title = element_text(hjust = 0.5), legend.position = "none")+
#   geom_line(aes(x = min.node.tune, color = "red"), linetype = "dashed")+
#   annotate(geom = "text",
#            label = paste("nodesize =",as.character(min.node.tune)),
#            x = min.node.tune,
#            y = prev.nodesize.rmse.tune[min.node.tune]+80,
#            vjust = 1.5,
#            angle = 90)
```

![REQM selon le nombre minimal d’observations par feuille de 1 à 40](forest_node_tune.jpg)

Dans la figure 10, la valeur minimale de `nodesize` est de 27.

Par conséquent, les hyperparamètres finaux pour le modèle de forêt aléatoire sont ceux décrits dans la table suivante.

```{r forest-parametre-final, cache=TRUE, echo=FALSE}
df.forest.final <- data.frame(
  "Hyperparamètre" = c("Nombre d'arbres", "Nombre de choix de variables à chaque noeud", "Nombre d'observation dans les noeuds terminaux"),
  "Valeur" = c(200, 8, 27)
)

knitr::kable(df.forest.final,caption = "Valeurs des hyperparamètres du modèle final")
```

```{r forest-model-final, cache=TRUE, echo=FALSE}
set.seed(963852)
foret.final <- randomForest(totalAmount~.,
                      data = dat.train,
                      nodesize = 27,
                      mtry = 8,
                      importance = T,
                      cp = 0,
                      sampsize = floor(0.5*nrow(data)),
                      ntree = 200,
                      )
```

\newpage

## *Gradient Boosting*

<!-- Danny -->

Nous commençons par entraîner un modèle GBM par validation croisée à 5 plis. Nous testons les valeurs de la taille maximale de chaque arbre $\{5,10,15\}$ et un nombre d'itérations de 1000 à 6000 par bonds de 1000, tout en utilisant un paramètre d'apprentissage, $\lambda = 0.01$.

```{r gradient, cache=TRUE, echo=FALSE}
# controles <- trainControl(method="cv", number= 5)
# 
# gbmGrille <-  expand.grid(n.trees = seq(1000,6000, 1000),
#                          interaction.depth = c(5,10,15),
#                          shrinkage = 0.01,
#                          n.minobsinnode = 50)
# 
# 
# set.seed(234137)
# 
# mod.gbm <- caret::train(totalAmount~.,
#                 data = dat.train,
#                 method = "gbm",
#                 trControl = controles,
#                 distribution = "gamma",
#                 tuneGrid = gbmGrille,
#                 verbose = FALSE,
#                 metric = "RMSE")
```

```{r gradient-plot, echo=FALSE, cache=TRUE}
# plot(mod.gbm)
```

![REQM selon la taille maximale de chaque arbre et le nombre d'itérations](gbm.jpg)

Nous voyons, dans la figure 11, qu'une profondeur de 15 et qu'environ 4000 arbres minimisent la REQM. Une profondeur de 15 semble suffisante pour capter les interactions entre les variables, sans trop faire exploser le temps de calcul. Il est difficile de voir le nombre idéal.

```{r gradient-tune, cache=TRUE, echo=FALSE}
# controles <- trainControl(method="cv", number= 5)
# 
# gbmGrille.tune <-  expand.grid(n.trees = seq(2500,4500, 500),
#                          interaction.depth = 15,
#                          shrinkage = 0.01,
#                          n.minobsinnode = 50)
# 
# 
# set.seed(234137)
# 
# mod.gbm.tune <- caret::train(totalAmount~.,
#                 data = dat.train,
#                 method = "gbm",
#                 trControl = controles,
#                 tuneGrid = gbmGrille.tune,
#                 verbose = FALSE)
```

```{r gradient-plot-tune, echo=FALSE, cache=TRUE}
#plot(mod.gbm.tune)
```

![REQM selon le nombre d'itérations](gbm_tune.jpg)

Après avoir fait une analyse plus précise, nous remarquons que 3000 itérations est optimal, et les paramètres finaux seront ceux décrits à la table suivante.

```{r gbm-parametre-final, cache=TRUE, echo=FALSE}
df.gbm.final <- data.frame(
  "Hyperparamètre" = c("Nombre d'arbres", "Profondeur de chaque arbre", "Taux d'apprentissage"),
  "Valeur" = c(3000, 15, 0.01)
)

knitr::kable(df.gbm.final,caption = "Valeurs des hyperparamètres du modèle final")
```

```{r gbm-model-final, cache=TRUE, echo=FALSE}
controles <- trainControl(method="cv", number= 5)

gbmGrille.fin <-  expand.grid(n.trees = 3000,
                         interaction.depth = 15,
                         shrinkage = 0.01,
                         n.minobsinnode = 5)

set.seed(963852)
gbm.final <- caret::train(totalAmount~.,
                data = dat.train,
                method = "gbm",
                trControl = controles,
                tuneGrid = gbmGrille.fin,
                verbose = FALSE)
```

\newpage

# Comparaison des modèles

<!-- Collectif -->

```{r predictions, echo=FALSE, warning=FALSE}
knn.pred <- pred.knn
arbre.pred <- treepred
bag.pred <- predict(bag.final, newdata = dat.test)
foret.pred <- predict(foret.final, newdata = dat.test)
gbm.pred <- predict(gbm.final, newdata = dat.test)
```

Nous avons décidé d'utiliser la racine carrée de l'erreur quadratique moyenne (REQM) afin de comparer la performance prédictive des différents modèles discutés dans le rapport. Les deux modèles retenues selon cette métrique sont le *bagging* et la forêt aléatoire puisque ce sont ceux qui ont la plus petite valeur de REQM.

```{r comparaison-EQM, echo=FALSE, warning=FALSE}
REQM <- function(pred) sqrt(mean((dat.test$totalAmount - pred)^2))

reqm.knn <- sqrt(mean((res_test - knn.pred)^2))

df.REQM <- data.frame("Modèle" = c("GLM Tweedie", "K plus proches voisins", "Arbre de décision", 
                                  "Bagging", "Forêt aléatoire", "Gradient boosting"),
                     "Valeur" = c(REQM(glm.pred), reqm.knn, REQM(arbre.pred), 
                                  REQM(bag.pred), REQM(foret.pred), REQM(gbm.pred)))

knitr::kable(df.REQM, 
             caption = "Valeurs des racines des erreurs quadratiques moyennes des modèles finaux")
```

Afin d'être sûr de nos choix, nous avons aussi mesuré l'erreur absolue moyenne(EAM).

```{r comparaison-EAM, echo=FALSE, warning=FALSE}
EAM <- function(pred) mean(abs(dat.test$totalAmount - pred))

eam.knn <- mean(abs(res_test - knn.pred))

df.EAM <- data.frame("Modèle" = c("GLM Tweedie", "K plus proches voisins", "Arbre de décision", 
                                  "Bagging", "Forêt aléatoire", "Gradient boosting"),
                     "Valeur" = c(EAM(glm.pred), eam.knn, EAM(arbre.pred), 
                                  EAM(bag.pred), EAM(foret.pred), EAM(gbm.pred)))

knitr::kable(df.EAM, 
             caption = "Valeurs des erreurs absolues moyennes des modèles finaux")
```

Les modèles ayant les petites valeurs sont le GLM Tweedie et la forêt aléatoire. Cependant, le GLM Tweedie avait la deuxième valeur plus grande pour le REQM. Puisque l'EAM est vraiment sensible aux valeurs aberrantes et que le *bagging* a encore une des plus petites valeurs cette métrique, nous avons décidé de choisir les modèles *bagging* et forêt aléatoire comme la REQM nous l'indiquait.  

\newpage

# Interprétation des meilleurs modèles 

<!-- Maryjane -->

```{r creation-mod.iml, echo=FALSE, cache=TRUE}
mod.iml.fa <- Predictor$new(foret.final)
mod.iml.bag <- Predictor$new(bag.final)
```

## Forêt aléatoire

```{r importance-foret aleatoire, echo=FALSE, warning=FALSE, cache=TRUE}
#imp.fa <- FeatureImp$new(mod.iml.fa, loss = "mse", compare = "difference")

#plot(imp.fa)
```

![Importances des variables pour la forêt aléatoire](importance_fa.jpg)

Selon la figure 13, la variable la plus importante pour le modèle forêt aléatoire est `totalCoverage`. Pour mieux comprendre son effet marginal sur la prévision, nous traçons le graphique de dépendance partielle et le graphique d'espérance conditionnelle individuelle.

```{r pdp-foret aleatoire, echo=FALSE, cache=TRUE}
#pdp.fa.tc <- FeatureEffect$new(mod.iml.fa, "totalCoverage", method = "pdp", grid.size = 30)

#ggplot(pdp.fa.tc$results, aes(x = totalCoverage, y = .value)) + 
#  geom_line() +
#  ylab("Prévisions") +
#  theme_minimal()
```

![Graphique de dépendance partielle pour la forêt aléatoire](pdp_fa.jpg)

```{r ice-foret aleatoire, echo=FALSE, cache=TRUE}
#ice.fa.tc <- FeatureEffect$new(mod.iml.fa, "totalCoverage", method = "ice", grid.size = 30)

#plot(ice.fa.tc)
```

![Graphique d'espérance conditionnelle individuelle pour la forêt aléatoire](ice_fa.jpg)

Comme nous pouvons le voir à la figure 14, les prévisions augmentent beaucoup pour les valeurs de `totalCoverage` de 0 à 1 000 000 et restent stables par après. Cette observation est logique puisque la variable `totalCoverage` représente le montant auquel l'assuré est couvert par son assurance et qu'un assuré ne peut pas réclamer plus de ce qu'il a de couvert.

Dans la figure 15, nous voyons avec les prévisions ayant une limite supérieure à 60 000 suivent la même logique, c'est-à-dire qu'elles augmentent beaucoup pour les valeurs de `totalCoverage` de 0 à 1 000 000 et restent stables par après. Cependant, il est beaucoup plus difficile de se prononcer pour les prévisions avec une limite inférieure à 60 000 puisqu'il y a beaucoup plus d'observations.

```{r interactions-foret aleatoire, echo=FALSE, cache=TRUE}
#set.seed(5660)
#int.fa.tc <- Interaction$new(mod.iml.fa, "totalCoverage")

#plot(int.fa.tc)

#pdp.fa.tc.ot <- FeatureEffect$new(mod.iml.fa, c("totalCoverage", "occupancyType"), 
#                                  method = "pdp", grid.size = 30)

#ggplot(pdp.fa.tc.ot$results, aes(x = totalCoverage, y = .value, color = occupancyType)) + 
#  geom_line() +
#  ylab("Prévisions") +
#  theme_minimal()
```

![Interactions des variables avec `totalCoverage` pour la forêt aléatoire](inter_fa.jpg)

![Interaction entre `totalCoverage` et `occupancyType` pour la forêt aléatoire](pdp_biv_fa.jpg)

La figure 16 nous présente les variables qui interagissent avec `totalCoverage` pour expliquer les prévisions. Nous observons que la plus grande interaction se produit avec `occupacyType`. Cette interaction est présentée à la figure 17. Nous observons que les prévisions selon le `totalCoverage` sont plus élevés pour les immeubles non résidentiels (`occupancyType` = 3). Pour les résidences familiales (`occupancyType` = 1) et les copropriétés résidentielles (`occupancyType` = 2), les prévisions sont très similaires.

Le modèle forêt aléatoire est un des meilleurs modèles puisque l'erreur OOB permet de garder un plus grand échantillon de donnée dans l'entrainement. Ainsi, le modèle utilise un échantillon plus représentatif du nombre de données que nous avons sur les inondations en Californie depuis 1974. Cependant, ce modèle est plus difficile à interpréter et il demande beaucoup de mémoire avec un grand nombre d'arbres.

\newpage

## *Bagging*

```{r importance-bagging, echo=FALSE, warning=FALSE}
#imp.bag <- FeatureImp$new(mod.iml.bag, loss = "mse", compare = "difference")

#plot(imp.bag)
```

![Importances des variables pour le *bagging*](importance_bag.jpg)

Selon la figure 18, la variable la plus importante pour le modèle *bagging* est aussi `totalCoverage`. Pour mieux comprendre son effet marginal sur la prévision, nous traçons le graphique de dépendance partielle et le graphique d'espérance conditionnelle individuelle.

```{r pdp-bagging, echo=FALSE, cache=TRUE}
#pdp.bag.tc <- FeatureEffect$new(mod.iml.bag, "totalCoverage", method = "pdp", grid.size = 30)

#ggplot(pdp.bag.tc$results, aes(x = totalCoverage, y = .value)) + 
#  geom_line() +
#  ylab("Prévisions") +
#  theme_minimal()
```

![Graphique de dépendance partielle pour le *bagging*](pdp_bag.jpg)

```{r ice-bagging, echo=FALSE, cache=TRUE}
#ice.bag.tc <- FeatureEffect$new(mod.iml.bag, "totalCoverage", method = "ice", grid.size = 30)

#plot(ice.bag.tc)
```

![Graphique d'espérance conditionnelle individuelle pour le *bagging*](ice_bag.jpg)

Comme nous pouvons le voir à la figure 19, les résultats sont très semblables au modèle de forêt aléatoire. Les prévisions augmentent beaucoup pour les valeurs de `totalCoverage` de 0 à 1 000 000 et restent stables par après. 

Dans la figure 20, encore ici, nous voyons avec les prévisions ayant une limite supérieure à 60 000 suivent la même logique, c'est-à-dire qu'elles augmentent beaucoup pour les valeurs de `totalCoverage` de 0 à 1 000 000 et restent stables par après. Il est encore difficile de se prononcer pour les prévisions dont la limite est inférieures à 60 000 puisqu'il y a beaucoup plus d'observations. Cependant, il y a beaucoup de prévisions ayant une limite inférieure à 75 000, très peu de 75 000 à 90 000 et un peu plus de 90 000 à 100 000.

```{r interactions-bagging, echo=FALSE, cache=TRUE}
#set.seed(5660)
#int.bag.tc <- Interaction$new(mod.iml.bag, "totalCoverage")

#plot(int.bag.tc)

#pdp.bag.tc.ot <- FeatureEffect$new(mod.iml.bag, c("totalCoverage", "occupancyType"), 
#                                  method = "pdp", grid.size = 30)

#ggplot(pdp.bag.tc.ot$results, aes(x = totalCoverage, y = .value, color = occupancyType)) + 
#  geom_line() +
#  ylab("Prévisions") +
#  theme_minimal()
```

![Interactions des variables avec `totalCoverage` pour le *bagging*](inter_bag.jpg)

![Interaction entre `totalCoverage` et `occupancyType` pour le *bagging*](pdp_biv_bag.jpg)

La figure 21 nous présente les variables qui interagissent avec `totalCoverage` pour expliquer les prévisions. Nous observons que la plus grande interaction se produit avec `occupancyType`.Cette interaction est présentée à la figure 22. Nous observons encore que les prévisions selon le `totalCoverage` sont plus élevés pour les immeubles non résidentiels (`occupancyType` = 3). Pour les résidences familiales (`occupancyType` = 1) et les copropriétés résidentielles (`occupancyType` = 2), les prévisions sont très similaires.

L'un des avantages du *bagging* est qu'il permet de réduire l'écart dans un algorithme d'apprentissage, ce qui est particulièrement utile avec des données de grande dimension comme c'est le cas pour ce rapport. Cependant, le *bagging* a tendance à être plus lent quand le nombre d'itérations augmente, ce qui le rend peu bien adapté aux applications en temps réel.

\newpage

# Conclusion

<!-- Henri -->

En conclusion, pour rappeler le but du présent rapport nous avons étudié la prédiction du montant des réclamations associées aux polices d'assurance inondation en Californie en ajustant plusieurs modèles prédictifs sur un jeu de données de la FEMA. Les résultats concernant les six modèles ajustés ont montré que la forêt aléatoire et le bagging sont les deux modèles les plus performants selon le critère RMSE, avec des valeurs respectives de 2705.454 et 2709.397.

Cependant, il convient de noter que notre approche présente certaines limites. Tout d'abord, nous avons considéré un nombre limité de variables explicatives, qui ne couvrent pas nécessairement tous les aspects pertinents pour la prédiction des montants de réclamation. De plus, l'ajustement des modèles prédictifs est un processus complexe, et les résultats peuvent varier selon les choix méthodologiques et les hyperparamètres choisis. Enfin, la qualité des données utilisées peut également affecter la performance des modèles prédictifs.

Malgré ces limites, notre étude montre le potentiel des modèles prédictifs pour la prédiction des montants de réclamation pour les polices d'assurance inondation en Californie. Des perspectives d'amélioration peuvent être envisagées, telles que collecte de nouvelles variables explicatives pertinentes ou l'utilisation de techniques de machine learning plus avancées pour un ajustement plus robuste des modèles prédictifs. En fin de compte, l'utilisation de modèles prédictifs peut aider les assureurs à mieux estimer les risques liés aux inondations en Californie et à fixer des primes adaptées, ce qui est crucial pour garantir la viabilité économique de l'assurance inondation dans cette région.

\newpage

# Bibliographie

The Federal Emergency Management Agency (2023). FIMA NFIP Redacted Claims - v1.

Récupéré de <https://www.fema.gov/openfema-data-page/fima-nfip-redacted-claims-v1>
